{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 데이터 전처리"
      ],
      "metadata": {
        "id": "YkcHTVUFId6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수문자, 구두점, 숫자 제거\n",
        "con_text['요약'] = con_text['요약'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 소문자 변환 전처리\n",
        "text = con_text.applymap(str.lower)\n",
        "#토큰화\n",
        "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
        "# 영문 불용어 처리\n",
        "stop_words = stopwords.words('english')\n",
        "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])\n",
        "# 토픽마다 중복되는 단어나 주제에 불필요한 단어 제거\n",
        "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words2)])\n",
        "# 표제어 추출\n",
        "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
        "# 길이 2 이하 제거\n",
        "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-MVg8R3mJ7_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexity, coherence"
      ],
      "metadata": {
        "id": "pfWQwtdAishY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import collections\n",
        "# 토큰화된 단어를 Dictionary 형태로 변환\n",
        "# doc2bow를 통해 corpus에 단어 저장\n",
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(tokenized_doc)\n",
        "print(dictionary.token2id)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
        "\n",
        "# perplexity 계산\n",
        "import matplotlib.pyplot as plt\n",
        "perplexity_values=[]\n",
        "for i in range(2,15):\n",
        "    ldamodel=gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=dictionary)\n",
        "    perplexity_values.append(ldamodel.log_perplexity(corpus))\n",
        "\n",
        "# 그래프 출력\n",
        "x=range(2,15)\n",
        "plt.plot(x, perplexity_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"perplexity score\")\n",
        "plt.show()\n",
        "# coherence 계산\n",
        "from gensim.models import CoherenceModel\n",
        "coherence_values=[]\n",
        "for i in range(2,15):\n",
        "    ldamodel=gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=dictionary)\n",
        "    coherence_model_lda=CoherenceModel(model=ldamodel, texts=tokenized_doc, dictionary=dictionary,topn=10)\n",
        "    coherence_lda=coherence_model_lda.get_coherence()\n",
        "    coherence_values.append(coherence_lda)\n",
        "# 그래프 출력\n",
        "x=range(2,15)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"coherence score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_LMvopWlU9do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic modeling\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qqh1cKqNZ6-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(tokenized_doc)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
        "\n",
        "# LDA 모델을 형성하여 11개의 토픽으로 군집화 진행\n",
        "NUM_TOPICS = 11 # 11개의 토픽, k=11\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
        "topics = ldamodel.print_topics(num_words=20)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "# Intertopic Distance Map, Top-30 Most Salient Terms 출력\n",
        "pip install pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "Kdfz3zlA6KSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SNA"
      ],
      "metadata": {
        "id": "sQv6K4wEzxCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = {} \n",
        "for tokens in tokenized_doc: \n",
        "    stopped_tokens = [i for i in list(set(tokens)) ] \n",
        "    for i,a in enumerate(stopped_tokens): \n",
        "        for b in stopped_tokens[i+1:]: \n",
        "            if a>b: \n",
        "                count[b,a] = count.get((b,a),0) + 1\n",
        "            else:\n",
        "                count[a,b] = count.get((a,b),0) + 1\n",
        "df = pd.DataFrame.from_dict(count, orient='index')\n",
        "list1=[]\n",
        "for i in range(len(df)):\n",
        "    list1.append([df.index[i][0], df.index[i][1], df[0][i]])\n",
        "df2 = pd.DataFrame(list1, columns=['term1','term2','freq'])\n",
        "df2 = df2.sort_values(by=['freq'], ascending=False) \n",
        "df3 = df2.reset_index(drop=True)\n",
        "import networkx as nx\n",
        "import operator\n",
        "G_centrality = nx.Graph() \n",
        "for ind in range(len(np.where(df3['freq']>=5)[0])):\n",
        "    G_centrality.add_edge(df3['term1'][ind],df3['term2'][ind],weight=int(df3['freq'][ind])\n",
        "cls = nx.closeness_centrality(G_centrality) \n",
        "#중심성 큰 순서대로 저장\n",
        "sorted_cls = sorted(cls.items(), key= operator.itemgetter(1), reverse = True)\n",
        "#단어 네트워크를 그려줄 Graph선언\n",
        "G=nx.Graph() \n",
        "for i in range(len(sorted_cls)):\n",
        "    G.add_node(sorted_cls[i][0], nodesize = sorted_cls[i][1])\n",
        "for ind in range(len(np.where(df3['freq']>=5)[0])):\n",
        "    G.add_weighted_edges_from([(df3['term1'][ind],df3['term2'][ind],int(df3['freq'][ind]))])\n",
        "# 노드 크기 조정\n",
        "sizes = [G.nodes[node]['nodesize']*2000 for node in G]\n",
        "options = {\n",
        "    'edge_color':'#FFDEA2',\n",
        "    'width':2,\n",
        "    'with_labels':True,\n",
        "    'font_weight':'regular'\n",
        "}\n",
        "nx.draw(G, node_size=sizes, pos=nx.spring_layout(G, k=4.5, iterations=100), **options)\n",
        "ax = plt.gca()\n",
        "ax.collections[0].set_edgecolor(\"#555555\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bDpYFSr968Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6QscsEoNDpU-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}